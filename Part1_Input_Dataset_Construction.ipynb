{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Input and Dataset Construction\n",
    "\n",
    "This notebook covers the complete workflow for preparing InSAR data for the Damage Proxy Mapping (DPM) analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Data Preprocessing (External)\n",
    "\n",
    "Before running this notebook, the **Sentinel-1 SAR data** must be preprocessed using `stackSentinel.py` (https://github.com/isce-framework/isce2/blob/main/contrib/stack/topsStack/stackSentinel.py), which automates:\n",
    "\n",
    "- **SAR Image Collection** - Multiple Sentinel-1 SAR images for the desired time period\n",
    "- **Baseline Estimation** - Determining geometric relationships between image pairs\n",
    "- **Multi-looking** - Reducing speckle noise\n",
    "- **Image Registration (Coregistration)** - Aligning images for interferometric analysis\n",
    "- **Filtering** - Noise reduction and phase filtering\n",
    "- **Interferogram Generation** - Creating interferograms from adjacent image pairs\n",
    "- **Coherence Map Generation** - Output: `filt_fine.cor` (filtered coherence files)\n",
    "\n",
    "## 2. This Notebook Workflow\n",
    "\n",
    "1. **SAR Preprocessing Overview** - Understanding the `stackSentinel.py` output\n",
    "2. **InSAR Data Processing Utilities** - Core functions for reading/writing ISCE2 files and coordinate transformations\n",
    "3. **Batch Cropping** - Extracting Region of Interest (ROI) from interferogram coherence files\n",
    "4. **Dataset Generation** - Building time series data structure and calculating phase standard deviation\n",
    "\n",
    "---\n",
    "\n",
    "## Input Files\n",
    "\n",
    "| File | Description | Source |\n",
    "|------|-------------|--------|\n",
    "| `filt_fine.cor` | Filtered coherence maps | `stackSentinel.py` output (final step after coregistration) |\n",
    "| `lat.rdr` | Latitude lookup table | `stackSentinel.py` geometry output |\n",
    "| `lon.rdr` | Longitude lookup table | `stackSentinel.py` geometry output |\n",
    "\n",
    "## Output Files\n",
    "\n",
    "| File | Description | Usage |\n",
    "|------|-------------|-------|\n",
    "| `data.npy` | Pre-event coherence time series (H, W, T) | LSTM training input |\n",
    "| `data_std.npy` | Phase standard deviation time series | LSTM training input |\n",
    "| `geninue.npy` | Co-event coherence data | Damage score calculation |\n",
    "| `geninue_std.npy` | Co-event phase standard deviation | Damage score calculation |\n",
    "| `dates.pkl` | Interferogram date pairs | Temporal feature extraction |\n",
    "| `lat_cropped.rdr` | Cropped latitude lookup table | Geocoding |\n",
    "| `lon_cropped.rdr` | Cropped longitude lookup table | Geocoding |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stackSentinel.py Overview\n",
    "\n",
    "`stackSentinel.py` is a key script in the ISCE processing chain, designed to automate the preprocessing of Sentinel-1 SAR data. It orchestrates various steps from initial data unpacking to generating interferograms, coherence maps, and applying necessary filters. Below is an overview of its workflow:\n",
    "\n",
    "### 1. Data Input and Unpacking\n",
    "The script begins by organizing and unpacking Sentinel-1 SAR data. It sets up directories and prepares the raw SAR data for further processing. This typically involves downloading and unpacking the raw Sentinel-1 products.\n",
    "\n",
    "### 2. Interferogram Calculation\n",
    "After unpacking, `stackSentinel.py` facilitates the calculation of interferograms between pairs of SAR images. These interferograms are the basis for detecting ground displacement.\n",
    "\n",
    "### 3. Coherence Map Generation\n",
    "The script generates coherence maps that quantify the correlation between SAR image pairs. High coherence values indicate stable surfaces, while low coherence values suggest changes or noisy areas.\n",
    "\n",
    "### 4. Phase Unwrapping and Topographic Phase Removal\n",
    "To process the interferograms, the script performs phase unwrapping, eliminating ambiguities in phase values. It also removes the topographic phase component, isolating the deformation signal from the topographic effects.\n",
    "\n",
    "### 5. Filtering and Refining\n",
    "Several filtering steps are included in the script to remove noise from the interferograms and coherence maps. This step enhances the quality of the data by smoothing and reducing phase errors.\n",
    "\n",
    "### 6. Coherence Thresholding and Final Filtering\n",
    "The script applies a coherence threshold, removing low-coherence regions, and further refines the coherence map by filtering out any remaining noise.\n",
    "\n",
    "### 7. Output: `filt_fine.cor`\n",
    "The final output of the script is a filtered and processed coherence map, stored in the `/merged/interferograms/` directory. This file, `filt_fine.cor`, serves as the foundation for any subsequent analysis of surface displacements or deformations.\n",
    "\n",
    "### Purpose\n",
    "The main purpose of `stackSentinel.py` is to automate the preprocessing steps for Sentinel-1 data, ensuring that the generated interferograms and coherence maps are of high quality and suitable for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# InSAR Data Processing Utilities\n",
    "\n",
    "This script provides several utilities to work with InSAR data, specifically focusing on geospatial transformations, file reading and writing, and data processing. It contains functions for converting geographic bounding boxes into SAR row/column indices, reading ISCE2 formatted files, and writing output data back to various file formats, including ISCE2 and GDAL. These utilities are particularly useful for processing and manipulating interferogram data from InSAR missions.\n",
    "\n",
    "## Functions\n",
    "\n",
    "### 1. `bbox2SAR(lat_min, lat_max, lon_min, lon_max, lat_data, lon_data)`\n",
    "\n",
    "Converts a geographic bounding box (latitude and longitude) into SAR (Synthetic Aperture Radar) row and column indices in the azimuth and range directions.\n",
    "\n",
    "* **Arguments**:\n",
    "\n",
    "  * `lat_min`: Minimum latitude.\n",
    "  * `lat_max`: Maximum latitude.\n",
    "  * `lon_min`: Minimum longitude.\n",
    "  * `lon_max`: Maximum longitude.\n",
    "  * `lat_data`: A 2D array of latitude values for the SAR grid.\n",
    "  * `lon_data`: A 2D array of longitude values for the SAR grid.\n",
    "* **Returns**:\n",
    "\n",
    "  * A list containing the SAR region in terms of row and column indices that correspond to the geographic bounding box.\n",
    "\n",
    "### 2. `read_isce_file(file)`\n",
    "\n",
    "Reads an ISCE2 formatted file (commonly used in InSAR data) and returns it as a NumPy array.\n",
    "\n",
    "* **Arguments**:\n",
    "\n",
    "  * `file`: Path to the ISCE2 formatted file (e.g., `.unw` file).\n",
    "* **Returns**:\n",
    "\n",
    "  * A NumPy array representing the data in the ISCE2 file.\n",
    "\n",
    "### 3. `write_gdal_file(arr, output_filepath, data_type=gdal.GDT_Float32)`\n",
    "\n",
    "Writes a NumPy array to a file using GDAL, specifically in the ENVI format. This function handles both 2D and 3D arrays.\n",
    "\n",
    "* **Arguments**:\n",
    "\n",
    "  * `arr`: The NumPy array to write to the file.\n",
    "  * `output_filepath`: The path to the output file.\n",
    "  * `data_type`: The data type for the output file (default is `gdal.GDT_Float32`).\n",
    "* **Returns**:\n",
    "\n",
    "  * None. The function writes the array to the specified file.\n",
    "\n",
    "### 4. `write_arr2file(arr, output_filepath)`\n",
    "\n",
    "Writes a NumPy array to an ISCE2 formatted file (such as `.unw`, `.cor`, or `.rdr`).\n",
    "\n",
    "* **Arguments**:\n",
    "\n",
    "  * `arr`: The NumPy array to convert and write.\n",
    "  * `output_filepath`: The path to the output ISCE2 file.\n",
    "* **Returns**:\n",
    "\n",
    "  * None. The function writes the array to the specified ISCE2 file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "from mintpy.utils.writefile import * \n",
    "from osgeo import gdal  \n",
    "import os, sys\n",
    "import numpy as np\n",
    "import math\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "## geobbox to SAR row/col in the azimuth&range direction \n",
    "def bbox2SAR(lat_min, lat_max, lon_min, lon_max, lat_data, lon_data):\n",
    "    #  convert geographic bounding box to SAR ROI indicies. (azimuth and range)\n",
    "    # <1> lat_min (float)     : minimum latitude\n",
    "    # <2> lat_max (float)     : maximum latitude\n",
    "    # <3> lon_min (float)     : minimum longitude\n",
    "    # <4> lon_max (float)     : maximum longitude\n",
    "    # <5> lat_data (np.array) : the latitude lookuptable lat.rdr  \n",
    "    # <6> lon_data (np.array) : the lontitude lookuptable lon.rdr\n",
    "    # <return> region_rec: return the SAR row col list \n",
    "    S, N, W, E = lat_min, lat_max, lon_min, lon_max\n",
    "    geo_coord = [W, N, E, S]\n",
    "    data_map =  (lon_data >= geo_coord[0])*(lon_data <= geo_coord[2])*(lat_data>=geo_coord[3])*(lat_data<=geo_coord[1])\n",
    "    region_list = np.argwhere(data_map==1)\n",
    "    region_rec = [10*math.floor(region_list[:,0].min()/10),10*math.ceil(region_list[:,0].max()/10),\\\n",
    "              10*math.floor(region_list[:,1].min()/10),10*math.ceil(region_list[:,1].max()/10)]\n",
    "    return region_rec\n",
    "    \n",
    "## convert ISCE2 formatted file to npArray\n",
    "def read_isce_file(file):\n",
    "    # convert a GDAL_realiable file (usually ISCE2 file) to a numpy array \n",
    "    # <1> file(str): a string-path of input file \n",
    "    _, ext = os.path.splitext(file)\n",
    "    ds = gdal.Open(file,gdal.GA_ReadOnly)\n",
    "    print(\"input dataset BandsCount : \", ds.RasterCount)\n",
    "    ## get the phase band for the unw data\n",
    "    if ext != \".unw\":band = ds.GetRasterBand(1)\n",
    "    else:band = band = ds.GetRasterBand(2)\n",
    "    data = np.expand_dims(band.ReadAsArray(), 2)\n",
    "    W, L, N = data.shape\n",
    "    loader = np.zeros([W, L, N], dtype=np.float32)\n",
    "    loader[:,:,0] = data[:,:,0]\n",
    "    return loader\n",
    "\n",
    "\n",
    "## there may be some problems in the funtion @\n",
    "## don`t use it \n",
    "def write_gdal_file(arr, output_filepath, data_type=gdal.GDT_Float32):\n",
    "    if len(arr.shape) == 2:\n",
    "        rows, cols = arr.shape\n",
    "        bands = 1\n",
    "        data_to_write = arr\n",
    "    elif len(arr.shape) == 3:\n",
    "        rows, cols, bands = arr.shape\n",
    "        data_to_write = arr[:, :, 0] if bands == 1 else arr\n",
    "    else:\n",
    "        raise ValueError(\"Array must be 2D or 3D\")\n",
    "    driver = gdal.GetDriverByName('ENVI')    \n",
    "    dataset = driver.Create(output_filepath, cols, rows, 1, data_type)\n",
    "    if dataset is None:\n",
    "        raise RuntimeError(f\"Could not create file: {output_filepath}\")\n",
    "    if len(arr.shape) == 2:\n",
    "        dataset.GetRasterBand(1).WriteArray(data_to_write)\n",
    "    else:\n",
    "        dataset.GetRasterBand(1).WriteArray(data_to_write[:, :])\n",
    "    dataset.FlushCache()\n",
    "    dataset = None\n",
    "    print(f\"GDAL write {output_filepath} finished\")\n",
    "\n",
    "\n",
    "## write np.array to the ISCE2 file according to the input array\n",
    "def write_arr2file(arr, output_filepath):\n",
    "    ## write np.array to the ISCE2 file according to the input array\n",
    "    ## <1> arr(numpy.array) : the numpy array to convert\n",
    "    ## <2> out_path (str)   : the output path of the arr2ISCEfile\n",
    "    mirror_dic = {\n",
    "        \".unw\": \"MOD_isce_unw\", \n",
    "        \".cor\": \"isce_cor\", \n",
    "        \".rdr\": \"isce_cor\", \n",
    "        \".int\": \"isce_int\",\n",
    "        \".full\": \"envi\"\n",
    "    }\n",
    "    _, ext = os.path.splitext(output_filepath)\n",
    "    if ext == \".full\":\n",
    "        write_gdal_file(arr, output_filepath)\n",
    "        return\n",
    "    if ext not in mirror_dic:\n",
    "        raise ValueError(f\"Unsupported file extension: {ext}\")\n",
    "    file_type = mirror_dic[ext]\n",
    "    write_isce_file(\n",
    "        data = arr[:,:,0],\n",
    "        out_file = output_filepath ,\n",
    "        file_type = file_type\n",
    "    )\n",
    "    print(f\"write {output_filepath} finished \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Crop of `filt_fine.cor` Files for InSAR Data\n",
    "\n",
    "This script provides a utility to batch crop `filt_fine.cor` files along with the corresponding latitude (`lat.rdr`) and longitude (`lon.rdr`) files from an InSAR dataset. The cropping is done based on a specified geographic region (latitude and longitude bounds), which is then converted into SAR image coordinates. The output files, which include the cropped versions of the `filt_fine.cor`, latitude, and longitude files, are saved to the designated output directory.\n",
    "\n",
    "## Key Functions\n",
    "\n",
    "1. **`batch_crop_filt_fine_cor()`**:\n",
    "\n",
    "   * This function processes all `filt_fine.cor` files in a given directory, crops them according to the specified geographic bounds, and saves the cropped files.\n",
    "   * It also crops the corresponding latitude and longitude files from the geometry reference path.\n",
    "\n",
    "2. **`crop_single_band_file()`**:\n",
    "\n",
    "   * Crops a single band file (such as a `filt_fine.cor` file) based on the provided coordinates and saves the cropped data.\n",
    "   * It checks whether the cropping region exceeds the data boundaries and adjusts accordingly.\n",
    "\n",
    "3. **`read_isce_file()`** and **`write_arr2file()`**:\n",
    "\n",
    "   * These helper functions are used for reading ISCE-formatted files and writing arrays to the corresponding output files (e.g., `.rdr`, `.cor`).\n",
    "\n",
    "## Script Overview\n",
    "\n",
    "* **Input**:\n",
    "\n",
    "  * A directory containing `filt_fine.cor` files (`base_path`).\n",
    "  * A reference geometry path containing `lat.rdr` and `lon.rdr` files (`geom_reference_path`).\n",
    "* **Output**:\n",
    "\n",
    "  * Cropped versions of the `filt_fine.cor`, latitude (`lat.rdr`), and longitude (`lon.rdr`) files saved to a specified output directory (`output_base_path`).\n",
    "\n",
    "### Geographic Region for Cropping\n",
    "\n",
    "The script is set to crop the data based on a fixed geographic bounding box:\n",
    "\n",
    "* **Latitude**: [42.625, 42.635]\n",
    "* **Longitude**: [13.28, 13.30]\n",
    "\n",
    "These bounds are used to define the region to be extracted from the InSAR dataset.\n",
    "\n",
    "## Example Workflow\n",
    "\n",
    "1. **Input Data**:\n",
    "\n",
    "   * `filt_fine.cor` files for interferogram data.\n",
    "   * `lat.rdr` and `lon.rdr` files for the corresponding latitude and longitude grids.\n",
    "\n",
    "2. **Cropping**:\n",
    "\n",
    "   * The latitude and longitude data are used to find the SAR region corresponding to the geographic bounding box.\n",
    "   * Each `filt_fine.cor` file is then cropped based on this region.\n",
    "\n",
    "3. **Output**:\n",
    "\n",
    "   * The cropped files are saved in the specified output directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch crop of filt_fine.cor files\n",
      "Input path: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms\n",
      "Geometry reference path: /data6/WORKDIR/AmatriceSenDT22/merged/geom_reference\n",
      "Output path: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped\n",
      "Using cropping region: Latitude[42.625, 42.635], Longitude[13.28, 13.3]\n",
      "Using lat file: /data6/WORKDIR/AmatriceSenDT22/merged/geom_reference/lat.rdr\n",
      "Using lon file: /data6/WORKDIR/AmatriceSenDT22/merged/geom_reference/lon.rdr\n",
      "input dataset BandsCount :  1\n",
      "input dataset BandsCount :  1\n",
      "Lat file shape: (2858, 26031, 1)\n",
      "Lon file shape: (2858, 26031, 1)\n",
      "Geographical coordinates converted to SAR coordinates: [950, 1060, 21330, 21870]\n",
      "SAR image range: Rows 950 to 1060, Columns 21330 to 21870\n",
      "SAR image size: 110 \u00d7 540 (Height \u00d7 Width)\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/lat_cropped.rdr\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/lat_cropped.rdr.xml\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/lat_cropped.rdr.vrt\n",
      "write /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/lat_cropped.rdr finished \n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/lon_cropped.rdr\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/lon_cropped.rdr.xml\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/lon_cropped.rdr.vrt\n",
      "write /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/lon_cropped.rdr finished \n",
      "Cropped lat file saved: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/lat_cropped.rdr, shape: (110, 540, 1)\n",
      "Cropped lon file saved: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/lon_cropped.rdr, shape: (110, 540, 1)\n",
      "Found 10 filt_fine.cor files\n",
      "\n",
      "Processing file 1/10: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/20160821_20160914/filt_fine.cor\n",
      "Extracted date information: 20160821_20160914\n",
      "input dataset BandsCount :  1\n",
      "Original data shape: (2858, 26031, 1)\n",
      "Cropped data shape: (110, 540, 1)\n",
      "Data range: Min=0.1605, Max=0.9012\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160821_20160914_filt_fine.cor\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160821_20160914_filt_fine.cor.xml\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160821_20160914_filt_fine.cor.vrt\n",
      "write /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160821_20160914_filt_fine.cor finished \n",
      "Successfully cropped: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160821_20160914_filt_fine.cor\n",
      "\n",
      "Processing file 2/10: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/20160809_20160821/filt_fine.cor\n",
      "Extracted date information: 20160809_20160821\n",
      "input dataset BandsCount :  1\n",
      "Original data shape: (2858, 26031, 1)\n",
      "Cropped data shape: (110, 540, 1)\n",
      "Data range: Min=0.1575, Max=0.9886\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160809_20160821_filt_fine.cor\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160809_20160821_filt_fine.cor.xml\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160809_20160821_filt_fine.cor.vrt\n",
      "write /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160809_20160821_filt_fine.cor finished \n",
      "Successfully cropped: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160809_20160821_filt_fine.cor\n",
      "\n",
      "Processing file 3/10: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/20160728_20160809/filt_fine.cor\n",
      "Extracted date information: 20160728_20160809\n",
      "input dataset BandsCount :  1\n",
      "Original data shape: (2858, 26031, 1)\n",
      "Cropped data shape: (110, 540, 1)\n",
      "Data range: Min=0.1639, Max=0.9864\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160728_20160809_filt_fine.cor\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160728_20160809_filt_fine.cor.xml\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160728_20160809_filt_fine.cor.vrt\n",
      "write /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160728_20160809_filt_fine.cor finished \n",
      "Successfully cropped: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160728_20160809_filt_fine.cor\n",
      "\n",
      "Processing file 4/10: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/20160716_20160728/filt_fine.cor\n",
      "Extracted date information: 20160716_20160728\n",
      "input dataset BandsCount :  1\n",
      "Original data shape: (2858, 26031, 1)\n",
      "Cropped data shape: (110, 540, 1)\n",
      "Data range: Min=0.1521, Max=0.9889\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160716_20160728_filt_fine.cor\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160716_20160728_filt_fine.cor.xml\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160716_20160728_filt_fine.cor.vrt\n",
      "write /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160716_20160728_filt_fine.cor finished \n",
      "Successfully cropped: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160716_20160728_filt_fine.cor\n",
      "\n",
      "Processing file 5/10: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/20160704_20160716/filt_fine.cor\n",
      "Extracted date information: 20160704_20160716\n",
      "input dataset BandsCount :  1\n",
      "Original data shape: (2858, 26031, 1)\n",
      "Cropped data shape: (110, 540, 1)\n",
      "Data range: Min=0.1593, Max=0.9428\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160704_20160716_filt_fine.cor\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160704_20160716_filt_fine.cor.xml\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160704_20160716_filt_fine.cor.vrt\n",
      "write /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160704_20160716_filt_fine.cor finished \n",
      "Successfully cropped: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160704_20160716_filt_fine.cor\n",
      "\n",
      "Processing file 6/10: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/20160610_20160704/filt_fine.cor\n",
      "Extracted date information: 20160610_20160704\n",
      "input dataset BandsCount :  1\n",
      "Original data shape: (2858, 26031, 1)\n",
      "Cropped data shape: (110, 540, 1)\n",
      "Data range: Min=0.1629, Max=0.9734\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160610_20160704_filt_fine.cor\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160610_20160704_filt_fine.cor.xml\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160610_20160704_filt_fine.cor.vrt\n",
      "write /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160610_20160704_filt_fine.cor finished \n",
      "Successfully cropped: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160610_20160704_filt_fine.cor\n",
      "\n",
      "Processing file 7/10: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/20160529_20160610/filt_fine.cor\n",
      "Extracted date information: 20160529_20160610\n",
      "input dataset BandsCount :  1\n",
      "Original data shape: (2858, 26031, 1)\n",
      "Cropped data shape: (110, 540, 1)\n",
      "Data range: Min=0.1555, Max=0.9886\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160529_20160610_filt_fine.cor\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160529_20160610_filt_fine.cor.xml\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160529_20160610_filt_fine.cor.vrt\n",
      "write /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160529_20160610_filt_fine.cor finished \n",
      "Successfully cropped: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160529_20160610_filt_fine.cor\n",
      "\n",
      "Processing file 8/10: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/20160517_20160529/filt_fine.cor\n",
      "Extracted date information: 20160517_20160529\n",
      "input dataset BandsCount :  1\n",
      "Original data shape: (2858, 26031, 1)\n",
      "Cropped data shape: (110, 540, 1)\n",
      "Data range: Min=0.1588, Max=0.9865\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160517_20160529_filt_fine.cor\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160517_20160529_filt_fine.cor.xml\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160517_20160529_filt_fine.cor.vrt\n",
      "write /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160517_20160529_filt_fine.cor finished \n",
      "Successfully cropped: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160517_20160529_filt_fine.cor\n",
      "\n",
      "Processing file 9/10: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/20160330_20160517/filt_fine.cor\n",
      "Extracted date information: 20160330_20160517\n",
      "input dataset BandsCount :  1\n",
      "Original data shape: (2858, 26031, 1)\n",
      "Cropped data shape: (110, 540, 1)\n",
      "Data range: Min=0.1534, Max=0.9807\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160330_20160517_filt_fine.cor\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160330_20160517_filt_fine.cor.xml\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160330_20160517_filt_fine.cor.vrt\n",
      "write /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160330_20160517_filt_fine.cor finished \n",
      "Successfully cropped: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160330_20160517_filt_fine.cor\n",
      "\n",
      "Processing file 10/10: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/20160306_20160330/filt_fine.cor\n",
      "Extracted date information: 20160306_20160330\n",
      "input dataset BandsCount :  1\n",
      "Original data shape: (2858, 26031, 1)\n",
      "Cropped data shape: (110, 540, 1)\n",
      "Data range: Min=0.1540, Max=0.9873\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160306_20160330_filt_fine.cor\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160306_20160330_filt_fine.cor.xml\n",
      "write file: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160306_20160330_filt_fine.cor.vrt\n",
      "write /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160306_20160330_filt_fine.cor finished \n",
      "Successfully cropped: /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160306_20160330_filt_fine.cor\n",
      "\n",
      "Batch cropping complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import traceback\n",
    "import re\n",
    "\n",
    "\n",
    "def batch_crop_filt_fine_cor(base_path, geom_reference_path, output_base_path):\n",
    "    \"\"\"\n",
    "    Batch crop all filt_fine.cor files and simultaneously crop corresponding lat and lon files.\n",
    "    \n",
    "    Parameters:\n",
    "    base_path: The base path containing filt_fine.cor files\n",
    "    geom_reference_path: The geometry reference path containing lat and lon files\n",
    "    output_base_path: The base path where cropped files will be saved\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the provided latitude and longitude range\n",
    "    lat_min, lat_max = 42.625, 42.635\n",
    "    lon_min, lon_max = 13.28, 13.30\n",
    "    \n",
    "    print(f\"Using cropping region: Latitude[{lat_min}, {lat_max}], Longitude[{lon_min}, {lon_max}]\")\n",
    "    \n",
    "    # Look for lat and lon files\n",
    "    lat_file = os.path.join(geom_reference_path, \"lat.rdr\")\n",
    "    lon_file = os.path.join(geom_reference_path, \"lon.rdr\")\n",
    "    \n",
    "    if not os.path.exists(lat_file):\n",
    "        # Try other possible file names\n",
    "        lat_files = [f for f in os.listdir(geom_reference_path) if \"lat\" in f.lower() and not f.startswith('.')]\n",
    "        if lat_files:\n",
    "            lat_file = os.path.join(geom_reference_path, lat_files[0])\n",
    "            print(f\"Using found latitude file: {lat_file}\")\n",
    "        else:\n",
    "            print(f\"Error: Could not find lat file in {geom_reference_path}\")\n",
    "            return\n",
    "    \n",
    "    if not os.path.exists(lon_file):\n",
    "        # Try other possible file names\n",
    "        lon_files = [f for f in os.listdir(geom_reference_path) if \"lon\" in f.lower() and not f.startswith('.')]\n",
    "        if lon_files:\n",
    "            lon_file = os.path.join(geom_reference_path, lon_files[0])\n",
    "            print(f\"Using found longitude file: {lon_file}\")\n",
    "        else:\n",
    "            print(f\"Error: Could not find lon file in {geom_reference_path}\")\n",
    "            return\n",
    "    \n",
    "    print(f\"Using lat file: {lat_file}\")\n",
    "    print(f\"Using lon file: {lon_file}\")\n",
    "    \n",
    "    # Read lat and lon data\n",
    "    lat_data = read_isce_file(lat_file)\n",
    "    lon_data = read_isce_file(lon_file)\n",
    "    print(f\"Lat file shape: {lat_data.shape}\")\n",
    "    print(f\"Lon file shape: {lon_data.shape}\")\n",
    "    \n",
    "    # Calculate the cropping region (SAR coordinates)\n",
    "    region_rec = bbox2SAR(lat_min, lat_max, lon_min, lon_max, lat_data, lon_data)\n",
    "    if len(region_rec) != 4:\n",
    "        print(\"Invalid region coordinates\")\n",
    "        return\n",
    "    \n",
    "    y_min, y_max, x_min, x_max = region_rec\n",
    "    height = y_max - y_min\n",
    "    width = x_max - x_min\n",
    "    print(f\"Geographical coordinates converted to SAR coordinates: [{y_min}, {y_max}, {x_min}, {x_max}]\")\n",
    "    print(f\"SAR image range: Rows {y_min} to {y_max}, Columns {x_min} to {x_max}\")\n",
    "    print(f\"SAR image size: {height} \u00d7 {width} (Height \u00d7 Width)\")\n",
    "    \n",
    "    # Check if the cropping region is valid\n",
    "    if height <= 0 or width <= 0:\n",
    "        print(\"Error: Invalid cropping region, height or width is 0\")\n",
    "        return\n",
    "    \n",
    "    # Crop the lat and lon files\n",
    "    lat_cropped = lat_data[y_min:y_max, x_min:x_max]\n",
    "    lon_cropped = lon_data[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    # Save the cropped lat and lon files\n",
    "    lat_output = os.path.join(output_base_path, \"lat_cropped.rdr\")\n",
    "    lon_output = os.path.join(output_base_path, \"lon_cropped.rdr\")\n",
    "    \n",
    "    write_arr2file(lat_cropped, lat_output)\n",
    "    write_arr2file(lon_cropped, lon_output)\n",
    "    \n",
    "    print(f\"Cropped lat file saved: {lat_output}, shape: {lat_cropped.shape}\")\n",
    "    print(f\"Cropped lon file saved: {lon_output}, shape: {lon_cropped.shape}\")\n",
    "    \n",
    "    # Walk through the directory structure and find filt_fine.cor files\n",
    "    cor_files = []\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file == \"filt_fine.cor\":\n",
    "                cor_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"Found {len(cor_files)} filt_fine.cor files\")\n",
    "    \n",
    "    # Process each file\n",
    "    for i, cor_file in enumerate(cor_files):\n",
    "        print(f\"\\nProcessing file {i+1}/{len(cor_files)}: {cor_file}\")\n",
    "        \n",
    "        try:\n",
    "            # Extract date information from file path\n",
    "            # Assuming the path format is: .../YYYYMMDD_YYYYMMDD/filt_fine.cor\n",
    "            date_match = re.search(r'(\\d{8}_\\d{8})', cor_file)\n",
    "            if date_match:\n",
    "                date_str = date_match.group(1)\n",
    "                print(f\"Extracted date information: {date_str}\")\n",
    "            else:\n",
    "                # If no date is found, use the parent directory name\n",
    "                parent_dir = os.path.basename(os.path.dirname(cor_file))\n",
    "                date_str = parent_dir\n",
    "                print(f\"Using parent directory name as date: {date_str}\")\n",
    "            \n",
    "            # Create the output file name\n",
    "            output_file = os.path.join(output_base_path, f\"{date_str}_filt_fine.cor\")\n",
    "            \n",
    "            # Check if the output file already exists\n",
    "            if os.path.exists(output_file):\n",
    "                print(f\"Skipping existing file: {output_file}\")\n",
    "                continue\n",
    "            \n",
    "            # Call the cropping function\n",
    "            crop_single_band_file(\n",
    "                cor_file, y_min, y_max, x_min, x_max, output_file\n",
    "            )\n",
    "            \n",
    "            print(f\"Successfully cropped: {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {cor_file}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "def crop_single_band_file(file, y_min, y_max, x_min, x_max, out_file):\n",
    "    \"\"\"Crop a single band file (such as filt_fine.cor)\"\"\"\n",
    "    \n",
    "    # Read data\n",
    "    data = read_isce_file(file)\n",
    "    print(f\"Original data shape: {data.shape}\")\n",
    "    \n",
    "    # Check if the cropping region exceeds data boundaries\n",
    "    data_height, data_width = data.shape[0], data.shape[1]\n",
    "    if y_max > data_height or x_max > data_width:\n",
    "        print(f\"Warning: Cropping region exceeds data boundaries, adjusting cropping region\")\n",
    "        y_max = min(y_max, data_height)\n",
    "        x_max = min(x_max, data_width)\n",
    "        print(f\"Adjusted SAR coordinates: [{y_min}, {y_max}, {x_min}, {x_max}]\")\n",
    "    \n",
    "    # Crop data\n",
    "    if len(data.shape) == 2:  # Single band data\n",
    "        data_crop = data[y_min:y_max, x_min:x_max]\n",
    "    elif len(data.shape) == 3:  # Multi-band data\n",
    "        data_crop = data[y_min:y_max, x_min:x_max, :]\n",
    "    else:\n",
    "        print(f\"Unsupported data dimensions: {len(data.shape)}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Cropped data shape: {data_crop.shape}\")\n",
    "    \n",
    "    # Calculate data statistics\n",
    "    if np.iscomplexobj(data_crop):\n",
    "        print(f\"Complex data range: Min magnitude={np.min(np.abs(data_crop)):.4f}, Max magnitude={np.max(np.abs(data_crop)):.4f}\")\n",
    "    else:\n",
    "        print(f\"Data range: Min={np.min(data_crop):.4f}, Max={np.max(data_crop):.4f}\")\n",
    "    \n",
    "    # Save cropped data\n",
    "    if not file.endswith(\"full\"):\n",
    "        write_arr2file(data_crop, out_file)\n",
    "    else:\n",
    "        write_gdal_file(data_crop, out_file)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    \n",
    "    # Set path parameters\n",
    "    base_path = \"/data6/WORKDIR/AmatriceSenDT22/merged/interferograms\"\n",
    "    geom_reference_path = \"/data6/WORKDIR/AmatriceSenDT22/merged/geom_reference\"\n",
    "    output_base_path = \"/data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped\"  # Output directory\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_base_path, exist_ok=True)\n",
    "    \n",
    "    print(\"Starting batch crop of filt_fine.cor files\")\n",
    "    print(f\"Input path: {base_path}\")\n",
    "    print(f\"Geometry reference path: {geom_reference_path}\")\n",
    "    print(f\"Output path: {output_base_path}\")\n",
    "    \n",
    "    # Execute batch cropping\n",
    "    batch_crop_filt_fine_cor(\n",
    "        base_path, geom_reference_path, output_base_path\n",
    "    )\n",
    "    \n",
    "    print(\"\\nBatch cropping complete!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InSAR Dataset Processing for Pre- and Post-Earthquake Data\n",
    "\n",
    "This script processes a batch of InSAR coherence files (`filt_fine.cor`), extracting both pre- and post-earthquake data to generate an InSAR dataset for further analysis. The main steps involve reading the coherence files, creating an InSAR timeseries (which contains pre-earthquake data), generating post-earthquake data, and saving the dataset along with calculated statistics (such as standard deviation). The result is a collection of processed data saved in `.npy` format, which can be used for machine learning, anomaly detection, or further scientific analysis.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. **File Handling**:\n",
    "\n",
    "   * The script finds all coherence files (`filt_fine.cor`) in a specified directory, sorts them by date, and processes them in chronological order.\n",
    "\n",
    "2. **Data Extraction**:\n",
    "\n",
    "   * Builds an InSAR timeseries using the selected coherence files before the earthquake event (specifically before 2016-08-24).\n",
    "   * Extracts the amplitude, phase, or real part of the data based on user choice.\n",
    "   * Generates post-earthquake data (`geninue.npy`) by using the last available coherence file after the earthquake event.\n",
    "\n",
    "3. **Standard Deviation Calculation**:\n",
    "\n",
    "   * Computes the standard deviation for each coherence band in the InSAR timeseries. This helps assess the coherence quality in each pixel.\n",
    "\n",
    "4. **Saving Processed Data**:\n",
    "\n",
    "   * Saves the processed InSAR timeseries (`data.npy`), corresponding dates (`dates.pkl`), post-earthquake data (`geninue.npy`), and standard deviation files (`data_std.npy` and `geninue_std.npy`).\n",
    "\n",
    "5. **Output Directory Structure**:\n",
    "\n",
    "   * The processed dataset is saved in a specified subfolder of the output directory for easy access.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Input**:\n",
    "\n",
    "   * Coherence files (`filt_fine.cor`) containing interferometric phase data.\n",
    "   * The Earthquake event date, used to divide the dataset into pre- and post-event data.\n",
    "\n",
    "2. **Processing**:\n",
    "\n",
    "   * The script reads and processes the coherence files, constructs an InSAR timeseries with pre-earthquake data, and calculates standard deviations for the data.\n",
    "   * After the earthquake event, the script extracts the post-earthquake coherence data (post-event `geninue.npy`).\n",
    "\n",
    "3. **Output**:\n",
    "\n",
    "   * The final output consists of:\n",
    "\n",
    "     * **InSAR timeseries** (pre-earthquake data).\n",
    "     * **Post-earthquake data** (`geninue.npy`).\n",
    "     * **Standard deviation** for both the pre- and post-earthquake data.\n",
    "\n",
    "### Example Directory Structure\n",
    "\n",
    "```\n",
    "/data6/WORKDIR/AmatriceSenDT22/\n",
    "\u251c\u2500\u2500 merged/\n",
    "\u2502   \u251c\u2500\u2500 interferograms/\n",
    "\u2502   \u2502   \u251c\u2500\u2500 cropped/   # Contains cropped coherence files\n",
    "\u2502   \u251c\u2500\u2500 interferograms/cropped/dataset/   # Output folder for saved dataset\n",
    "\u2502   \u251c\u2500\u2500 geom_reference/  # Geometry reference files (lat, lon, etc.)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 cropped .cor files\n",
      "2016-03-06 20160306_20160330 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160306_20160330_filt_fine.cor\n",
      "2016-03-30 20160330_20160517 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160330_20160517_filt_fine.cor\n",
      "2016-05-17 20160517_20160529 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160517_20160529_filt_fine.cor\n",
      "2016-05-29 20160529_20160610 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160529_20160610_filt_fine.cor\n",
      "2016-06-10 20160610_20160704 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160610_20160704_filt_fine.cor\n",
      "2016-07-04 20160704_20160716 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160704_20160716_filt_fine.cor\n",
      "2016-07-16 20160716_20160728 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160716_20160728_filt_fine.cor\n",
      "2016-07-28 20160728_20160809 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160728_20160809_filt_fine.cor\n",
      "2016-08-09 20160809_20160821 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160809_20160821_filt_fine.cor\n",
      "2016-08-21 20160821_20160914 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160821_20160914_filt_fine.cor\n",
      "Input dataset BandsCount: 1\n",
      "Data shape: (110, 540, 1)\n",
      "First image 20160306_20160330 shape: (110, 540, 1), dtype: float32\n",
      "[1/9] Reading 20160306_20160330 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160306_20160330_filt_fine.cor\n",
      "Input dataset BandsCount: 1\n",
      "Data shape: (110, 540, 1)\n",
      "[2/9] Reading 20160330_20160517 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160330_20160517_filt_fine.cor\n",
      "Input dataset BandsCount: 1\n",
      "Data shape: (110, 540, 1)\n",
      "[3/9] Reading 20160517_20160529 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160517_20160529_filt_fine.cor\n",
      "Input dataset BandsCount: 1\n",
      "Data shape: (110, 540, 1)\n",
      "[4/9] Reading 20160529_20160610 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160529_20160610_filt_fine.cor\n",
      "Input dataset BandsCount: 1\n",
      "Data shape: (110, 540, 1)\n",
      "[5/9] Reading 20160610_20160704 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160610_20160704_filt_fine.cor\n",
      "Input dataset BandsCount: 1\n",
      "Data shape: (110, 540, 1)\n",
      "[6/9] Reading 20160704_20160716 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160704_20160716_filt_fine.cor\n",
      "Input dataset BandsCount: 1\n",
      "Data shape: (110, 540, 1)\n",
      "[7/9] Reading 20160716_20160728 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160716_20160728_filt_fine.cor\n",
      "Input dataset BandsCount: 1\n",
      "Data shape: (110, 540, 1)\n",
      "[8/9] Reading 20160728_20160809 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160728_20160809_filt_fine.cor\n",
      "Input dataset BandsCount: 1\n",
      "Data shape: (110, 540, 1)\n",
      "[9/9] Reading 20160809_20160821 -> /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/20160809_20160821_filt_fine.cor\n",
      "Input dataset BandsCount: 1\n",
      "Data shape: (110, 540, 1)\n",
      "Final timeseries shape: (110, 540, 9) (H, W, T-1)\n",
      "Input dataset BandsCount: 1\n",
      "Data shape: (110, 540, 1)\n",
      "Dataset saved to /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/dataset/data.npy\n",
      "Dates list saved to /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/dataset/dates.pkl\n",
      "Saved geninue.npy to /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/dataset/geninue.npy\n",
      "Saved standard deviation data to /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/dataset/data_std.npy\n",
      "Saved geninue_std.npy to /data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped/dataset/geninue_std.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "import re\n",
    "from osgeo import gdal\n",
    "\n",
    "# Set directory paths\n",
    "cropped_dir = \"/data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped\"\n",
    "output_dir = \"/data6/WORKDIR/AmatriceSenDT22/merged/interferograms/cropped\"\n",
    "next_date = \"20160821_20160902\"  # Predicted time period\n",
    "\n",
    "# Earthquake event date\n",
    "event_date = datetime.datetime(2016, 8, 24)\n",
    "\n",
    "def find_cor_files_sorted(cropped_dir):\n",
    "    \"\"\"\n",
    "    Find all *_filt_fine.cor files under cropped_dir,\n",
    "    sort by the start date, and return [(start_datetime, date_str, full_path), ...]\n",
    "    where date_str is in the format '20160716_20160728'.\n",
    "    \"\"\"\n",
    "    file_infos = []\n",
    "    for root, dirs, files in os.walk(cropped_dir):\n",
    "        for fname in files:\n",
    "            if not fname.endswith(\"filt_fine.cor\"):\n",
    "                continue\n",
    "            m = re.search(r\"(\\d{8}_\\d{8})\", fname)\n",
    "            if not m:\n",
    "                print(f\"Skipping file without date pattern: {os.path.join(root, fname)}\")\n",
    "                continue\n",
    "            date_str = m.group(1)\n",
    "            start_str, end_str = date_str.split(\"_\")\n",
    "            start_dt = datetime.datetime.strptime(start_str, \"%Y%m%d\")\n",
    "            full_path = os.path.join(root, fname)\n",
    "            file_infos.append((start_dt, date_str, full_path))\n",
    "\n",
    "    # Sort by start date\n",
    "    file_infos.sort(key=lambda x: x[0])\n",
    "    print(f\"Found {len(file_infos)} cropped .cor files\")\n",
    "    for dt, dstr, path in file_infos:\n",
    "        print(dt.date(), dstr, \"->\", path)\n",
    "    return file_infos\n",
    "\n",
    "def read_isce_file(file):\n",
    "    \"\"\"\n",
    "    Read ISCE2 formatted files (such as .unw files) and return as a NumPy array.\n",
    "    \"\"\"\n",
    "    _, ext = os.path.splitext(file)\n",
    "    ds = gdal.Open(file, gdal.GA_ReadOnly)\n",
    "    print(\"Input dataset BandsCount:\", ds.RasterCount)\n",
    "\n",
    "    # If it's a .unw file, get the phase band data\n",
    "    if ext != \".unw\":\n",
    "        band = ds.GetRasterBand(1)\n",
    "    else:\n",
    "        band = ds.GetRasterBand(2)\n",
    "        \n",
    "    data = np.expand_dims(band.ReadAsArray(), 2)\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    W, L, N = data.shape\n",
    "    loader = np.zeros([W, L, N], dtype=np.float32)\n",
    "    loader[:, :, 0] = data[:, :, 0]\n",
    "    return loader\n",
    "\n",
    "def build_insar_timeseries_from_cor(file_infos, use=\"amplitude\"):\n",
    "    \"\"\"\n",
    "    Build InSAR timeseries (H, W, T-1) from sorted .cor files\n",
    "    \n",
    "    use:\n",
    "      - \"amplitude\": Use |cor|\n",
    "      - \"phase\": Use np.angle(cor)\n",
    "      - \"real\": Use np.real(cor)\n",
    "    \"\"\"\n",
    "    if len(file_infos) == 0:\n",
    "        raise RuntimeError(\"No .cor files found\")\n",
    "\n",
    "    # Read the first file to determine size\n",
    "    _, first_datestr, first_path = file_infos[0]\n",
    "    first_data = read_isce_file(first_path)  # Should return 2D array (H, W), possibly complex\n",
    "    print(f\"First image {first_datestr} shape: {first_data.shape}, dtype: {first_data.dtype}\")\n",
    "\n",
    "    # If first_data is a 3D array (H, W, 1), use [:2] to get H and W\n",
    "    H, W = first_data.shape[:2]\n",
    "    T = len(file_infos)\n",
    "\n",
    "    # The last dimension will be T-1, as the data before the earthquake\n",
    "    timeseries = np.zeros((H, W, T-1), dtype=np.float32)\n",
    "    dates = []\n",
    "\n",
    "    # Use only the first T-1 files, the last one is for geninue.npy\n",
    "    for t, (start_dt, date_str, path) in enumerate(file_infos[:-1]):  # Use only the first T-1 files\n",
    "        print(f\"[{t+1}/{T-1}] Reading {date_str} -> {path}\")\n",
    "        arr = read_isce_file(path)\n",
    "\n",
    "        if arr.shape[:2] != (H, W):\n",
    "            raise ValueError(f\"File {path} shape {arr.shape} does not match the first image {first_data.shape}\")\n",
    "\n",
    "        if np.iscomplexobj(arr):\n",
    "            if use == \"amplitude\":\n",
    "                arr_use = np.abs(arr)\n",
    "            elif use == \"phase\":\n",
    "                arr_use = np.angle(arr)\n",
    "            elif use == \"real\":\n",
    "                arr_use = np.real(arr)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown 'use' option: {use}\")\n",
    "        else:\n",
    "            arr_use = arr\n",
    "\n",
    "        # Convert to a 2D array (H, W), and store it in the timeseries\n",
    "        timeseries[:, :, t] = arr_use[:, :, 0].astype(np.float32)\n",
    "        dates.append(date_str)\n",
    "\n",
    "    print(f\"Final timeseries shape: {timeseries.shape} (H, W, T-1)\")\n",
    "    return timeseries, dates\n",
    "\n",
    "def calculate_std_from_cor(cor, chunk_size=50):\n",
    "    \"\"\"\n",
    "    Calculate standard deviation from coherence image\n",
    "    Calculate standard deviation for each band\n",
    "    \"\"\"\n",
    "    rows, cols, bands = cor.shape\n",
    "    result = np.full_like(cor, np.nan, dtype=np.float32)\n",
    "    \n",
    "    # Add numerical stability factor\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    # Iterate over the whole image in chunks (no overlap)\n",
    "    for i in range(0, rows, chunk_size):\n",
    "        for j in range(0, cols, chunk_size):\n",
    "            # Calculate the boundaries for the current chunk\n",
    "            end_i = min(i + chunk_size, rows)\n",
    "            end_j = min(j + chunk_size, cols)\n",
    "            \n",
    "            # Extract current chunk data\n",
    "            chunk = cor[i:end_i, j:end_j, :]  # Process all bands\n",
    "            \n",
    "            # Calculate the standard deviation for each band separately\n",
    "            for band in range(bands):\n",
    "                band_chunk = chunk[:, :, band]\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    # Calculate standard deviation: std = sqrt((1 - cor^2) / 2*cor^2)\n",
    "                    denominator = band_chunk**2\n",
    "                    valid_mask = (denominator > epsilon)\n",
    "                    std_chunk = np.where(valid_mask,\n",
    "                                         np.sqrt((1 - denominator) / (2*denominator)),\n",
    "                                         0.0)\n",
    "                \n",
    "                # Store the standard deviation in the corresponding position\n",
    "                result[i:end_i, j:end_j, band] = std_chunk\n",
    "    \n",
    "    # Retain original invalid regions\n",
    "    nan_mask = np.isnan(cor)\n",
    "    zero_mask = (cor == 0)\n",
    "    result[nan_mask] = np.nan\n",
    "    result[zero_mask] = 0.0\n",
    "    \n",
    "    return result\n",
    "\n",
    "def save_dataset(timeseries, dates, output_subfolder, geninue_data=None):\n",
    "    \"\"\"\n",
    "    Save dataset (.npy and .pkl) to specified subfolder\n",
    "    \"\"\"\n",
    "    os.makedirs(output_subfolder, exist_ok=True)\n",
    "    \n",
    "    # Save data\n",
    "    data_path = os.path.join(output_subfolder, \"data.npy\")\n",
    "    np.save(data_path, timeseries)\n",
    "    print(f\"Dataset saved to {data_path}\")\n",
    "\n",
    "    # Save dates\n",
    "    dates_path = os.path.join(output_subfolder, \"dates.pkl\")\n",
    "    with open(dates_path, \"wb\") as f:\n",
    "        pickle.dump(dates, f)\n",
    "    print(f\"Dates list saved to {dates_path}\")\n",
    "\n",
    "    # Save geninue.npy (post-earthquake cor)\n",
    "    if geninue_data is not None:\n",
    "        geninue_path = os.path.join(output_subfolder, \"geninue.npy\")\n",
    "        np.save(geninue_path, geninue_data)\n",
    "        print(f\"Saved geninue.npy to {geninue_path}\")\n",
    "\n",
    "    # Save standard deviation data\n",
    "    data_std = calculate_std_from_cor(timeseries)\n",
    "    data_std_path = os.path.join(output_subfolder, \"data_std.npy\")\n",
    "    np.save(data_std_path, data_std)\n",
    "    print(f\"Saved standard deviation data to {data_std_path}\")\n",
    "\n",
    "    if geninue_data is not None:\n",
    "        # Ensure geninue_data is a 3D array\n",
    "        if geninue_data.ndim == 2:\n",
    "            geninue_data = np.expand_dims(geninue_data, axis=-1)  # Convert to a 3D array, shape (rows, cols, 1)\n",
    "        \n",
    "        geninue_std = calculate_std_from_cor(geninue_data)\n",
    "        geninue_std_path = os.path.join(output_subfolder, \"geninue_std.npy\")\n",
    "        np.save(geninue_std_path, geninue_std)\n",
    "        print(f\"Saved geninue_std.npy to {geninue_std_path}\")\n",
    "\n",
    "def main():\n",
    "    # Get cropped files\n",
    "    file_infos = find_cor_files_sorted(cropped_dir)\n",
    "\n",
    "    # Select training set (before 20160824)\n",
    "    train_file_infos = [info for info in file_infos if info[0] < event_date]\n",
    "\n",
    "    # Build training set data\n",
    "    timeseries, dates = build_insar_timeseries_from_cor(train_file_infos, use=\"amplitude\")\n",
    "\n",
    "    # Generate geninue.npy (post-earthquake cor data, i.e., the last time interference pair's cor)\n",
    "    _, geninue_date_str, geninue_path = file_infos[-1]  # Last time period data\n",
    "    geninue_data = read_isce_file(geninue_path)\n",
    "    geninue_data = np.abs(geninue_data[:, :, 0])  # Get amplitude and remove the last dimension\n",
    "\n",
    "    # Save dataset\n",
    "    output_subfolder = os.path.join(output_dir, \"dataset\")\n",
    "    save_dataset(timeseries, dates, output_subfolder, geninue_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isce_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}